import pandas as pd
import requests
import time
import sys
import os
import urllib3
import datetime
import json
import sqlite3
import threading
import schedule
from pathlib import Path
from typing import Dict, List, Optional
import logging

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class HolderCollectionTask:
    """Holderæ•°æ®é‡‡é›†ä»»åŠ¡ç±»"""
    
    def __init__(self, task_id: str, token_address: str, token_symbol: str, 
                 chain: str, interval_hours: int, max_records: int = 1000,
                 description: str = ""):
        self.task_id = task_id
        self.token_address = token_address
        self.token_symbol = token_symbol
        self.chain = chain
        self.interval_hours = interval_hours
        self.max_records = max_records
        self.description = description
        self.created_at = datetime.datetime.now()
        self.last_run = None
        self.next_run = None
        self.status = "active"  # active, paused, stopped
        self.total_collections = 0
        self.last_error = None
        
    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'task_id': self.task_id,
            'token_address': self.token_address,
            'token_symbol': self.token_symbol,
            'chain': self.chain,
            'interval_hours': self.interval_hours,
            'max_records': self.max_records,
            'description': self.description,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'last_run': self.last_run.isoformat() if self.last_run else None,
            'next_run': self.next_run.isoformat() if self.next_run else None,
            'status': self.status,
            'total_collections': self.total_collections,
            'last_error': self.last_error
        }
    
    @classmethod
    def from_dict(cls, data: Dict):
        """ä»å­—å…¸åˆ›å»ºä»»åŠ¡å¯¹è±¡"""
        task = cls(
            task_id=data['task_id'],
            token_address=data['token_address'],
            token_symbol=data['token_symbol'],
            chain=data['chain'],
            interval_hours=data['interval_hours'],
            max_records=data.get('max_records', 1000),
            description=data.get('description', "")
        )
        
        if data.get('created_at'):
            task.created_at = datetime.datetime.fromisoformat(data['created_at'])
        if data.get('last_run'):
            task.last_run = datetime.datetime.fromisoformat(data['last_run'])
        if data.get('next_run'):
            task.next_run = datetime.datetime.fromisoformat(data['next_run'])
            
        task.status = data.get('status', 'active')
        task.total_collections = data.get('total_collections', 0)
        task.last_error = data.get('last_error')
        
        return task


class HolderDataCollector:
    """Holderæ•°æ®è‡ªåŠ¨é‡‡é›†å™¨"""
    
    def __init__(self, db_path: str = "holders_snapshots.db"):
        self.db_path = db_path
        self.tasks: Dict[str, HolderCollectionTask] = {}
        self.scheduler_thread = None
        self.is_running = False
        self.init_database()
        self.load_tasks()
        
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºä»»åŠ¡è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS collection_tasks (
                task_id TEXT PRIMARY KEY,
                token_address TEXT NOT NULL,
                token_symbol TEXT NOT NULL,
                chain TEXT NOT NULL,
                interval_hours INTEGER NOT NULL,
                max_records INTEGER DEFAULT 1000,
                description TEXT,
                created_at TIMESTAMP,
                last_run TIMESTAMP,
                next_run TIMESTAMP,
                status TEXT DEFAULT 'active',
                total_collections INTEGER DEFAULT 0,
                last_error TEXT
            )
        ''')
        
        # åˆ›å»ºå¿«ç…§æ•°æ®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS holder_snapshots (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                task_id TEXT NOT NULL,
                snapshot_time TIMESTAMP NOT NULL,
                holder_address TEXT NOT NULL,
                token_address TEXT NOT NULL,
                balance TEXT NOT NULL,
                percentage REAL,
                rank_position INTEGER,
                value_usd REAL,
                FOREIGN KEY (task_id) REFERENCES collection_tasks (task_id)
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_snapshots_task_time 
            ON holder_snapshots (task_id, snapshot_time)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_snapshots_address 
            ON holder_snapshots (holder_address)
        ''')
        
        conn.commit()
        conn.close()
        logger.info(f"âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {self.db_path}")
    
    def add_task(self, task: HolderCollectionTask) -> bool:
        """æ·»åŠ é‡‡é›†ä»»åŠ¡"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å­˜åœ¨
            cursor.execute("SELECT task_id FROM collection_tasks WHERE task_id = ?", (task.task_id,))
            if cursor.fetchone():
                logger.warning(f"âš ï¸ ä»»åŠ¡ {task.task_id} å·²å­˜åœ¨")
                return False
            
            # æ’å…¥ä»»åŠ¡
            cursor.execute('''
                INSERT INTO collection_tasks 
                (task_id, token_address, token_symbol, chain, interval_hours, max_records, 
                 description, created_at, status, total_collections)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                task.task_id, task.token_address, task.token_symbol, task.chain,
                task.interval_hours, task.max_records, task.description,
                task.created_at, task.status, task.total_collections
            ))
            
            conn.commit()
            conn.close()
            
            # æ·»åŠ åˆ°å†…å­˜
            self.tasks[task.task_id] = task
            
            # å®‰æ’å®šæ—¶ä»»åŠ¡
            self.schedule_task(task)
            
            logger.info(f"âœ… æˆåŠŸæ·»åŠ é‡‡é›†ä»»åŠ¡: {task.task_id}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ ä»»åŠ¡å¤±è´¥: {e}")
            return False
    
    def remove_task(self, task_id: str) -> bool:
        """åˆ é™¤é‡‡é›†ä»»åŠ¡"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # åˆ é™¤ä»»åŠ¡ï¼ˆä½†ä¿ç•™å†å²æ•°æ®ï¼‰
            cursor.execute("DELETE FROM collection_tasks WHERE task_id = ?", (task_id,))
            conn.commit()
            conn.close()
            
            # ä»å†…å­˜ä¸­ç§»é™¤
            if task_id in self.tasks:
                del self.tasks[task_id]
            
            # å–æ¶ˆå®šæ—¶ä»»åŠ¡
            schedule.clear(task_id)
            
            logger.info(f"âœ… æˆåŠŸåˆ é™¤ä»»åŠ¡: {task_id}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤ä»»åŠ¡å¤±è´¥: {e}")
            return False
    
    def pause_task(self, task_id: str) -> bool:
        """æš‚åœä»»åŠ¡"""
        return self.update_task_status(task_id, "paused")
    
    def resume_task(self, task_id: str) -> bool:
        """æ¢å¤ä»»åŠ¡"""
        return self.update_task_status(task_id, "active")
    
    def update_task_status(self, task_id: str, status: str) -> bool:
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute(
                "UPDATE collection_tasks SET status = ? WHERE task_id = ?",
                (status, task_id)
            )
            conn.commit()
            conn.close()
            
            if task_id in self.tasks:
                self.tasks[task_id].status = status
            
            logger.info(f"âœ… ä»»åŠ¡ {task_id} çŠ¶æ€æ›´æ–°ä¸º: {status}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def load_tasks(self):
        """ä»æ•°æ®åº“åŠ è½½ä»»åŠ¡"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("SELECT * FROM collection_tasks")
            rows = cursor.fetchall()
            columns = [description[0] for description in cursor.description]
            
            for row in rows:
                task_data = dict(zip(columns, row))
                task = HolderCollectionTask.from_dict(task_data)
                self.tasks[task.task_id] = task
                
                # ä¸ºæ´»è·ƒä»»åŠ¡å®‰æ’å®šæ—¶æ‰§è¡Œ
                if task.status == 'active':
                    self.schedule_task(task)
            
            conn.close()
            logger.info(f"âœ… åŠ è½½äº† {len(self.tasks)} ä¸ªé‡‡é›†ä»»åŠ¡")
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ä»»åŠ¡å¤±è´¥: {e}")
    
    def schedule_task(self, task: HolderCollectionTask):
        """å®‰æ’å®šæ—¶ä»»åŠ¡"""
        if task.status != 'active':
            return
        
        # æ¸…é™¤ç°æœ‰çš„åŒåä»»åŠ¡
        schedule.clear(task.task_id)
            
        # æ ¹æ®é—´éš”æ—¶é—´å®‰æ’ä»»åŠ¡
        if task.interval_hours == 1:
            job = schedule.every().hour.do(self.run_collection, task.task_id).tag(task.task_id)
        elif task.interval_hours == 4:
            job = schedule.every(4).hours.do(self.run_collection, task.task_id).tag(task.task_id)
        elif task.interval_hours == 12:
            job = schedule.every(12).hours.do(self.run_collection, task.task_id).tag(task.task_id)
        elif task.interval_hours == 24:
            job = schedule.every(24).hours.do(self.run_collection, task.task_id).tag(task.task_id)
        else:
            # è‡ªå®šä¹‰é—´éš”
            job = schedule.every(task.interval_hours).hours.do(self.run_collection, task.task_id).tag(task.task_id)
        
        # ä»scheduleè·å–å®é™…çš„ä¸‹æ¬¡è¿è¡Œæ—¶é—´
        task.next_run = job.next_run
        logger.info(f"ğŸ“… ä»»åŠ¡ {task.task_id} å·²å®‰æ’ï¼Œä¸‹æ¬¡è¿è¡Œ: {task.next_run}")
    
    def run_collection(self, task_id: str):
        """æ‰§è¡Œæ•°æ®é‡‡é›†"""
        if task_id not in self.tasks:
            logger.warning(f"âš ï¸ ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
            return
        
        task = self.tasks[task_id]
        if task.status != 'active':
            logger.info(f"â¸ï¸ ä»»åŠ¡ {task_id} çŠ¶æ€ä¸º {task.status}ï¼Œè·³è¿‡æ‰§è¡Œ")
            return
        
        logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œé‡‡é›†ä»»åŠ¡: {task_id}")
        
        try:
            # è·å–holderæ•°æ®
            holders_data = self.fetch_holders_data(task)
            
            if holders_data:
                # ä¿å­˜åˆ°æ•°æ®åº“
                self.save_snapshot(task_id, holders_data)
                
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                task.last_run = datetime.datetime.now()
                task.total_collections += 1
                task.last_error = None
                # ä»scheduleè·å–æ›´æ–°åçš„ä¸‹æ¬¡è¿è¡Œæ—¶é—´
                self.update_task_next_run_from_schedule(task_id)
                
                self.update_task_in_db(task)
                
                logger.info(f"âœ… ä»»åŠ¡ {task_id} æ‰§è¡Œå®Œæˆï¼Œé‡‡é›† {len(holders_data)} æ¡è®°å½•")
            else:
                # å³ä½¿æœªè·å–åˆ°æ•°æ®ä¹Ÿè¦æ›´æ–°æ—¶é—´ï¼Œç¡®ä¿ä»»åŠ¡ç»§ç»­è°ƒåº¦
                task.last_run = datetime.datetime.now()
                task.next_run = task.last_run + datetime.timedelta(hours=task.interval_hours)
                task.last_error = "æœªè·å–åˆ°æ•°æ®"
                self.update_task_in_db(task)
                logger.warning(f"âš ï¸ ä»»åŠ¡ {task_id} æœªè·å–åˆ°æ•°æ®")
                
        except Exception as e:
            error_msg = str(e)
            task.last_error = error_msg
            # å³ä½¿å¤±è´¥ä¹Ÿè¦æ›´æ–°last_runå’Œnext_runï¼Œç¡®ä¿ä»»åŠ¡èƒ½ç»§ç»­è°ƒåº¦
            task.last_run = datetime.datetime.now()
            task.next_run = task.last_run + datetime.timedelta(hours=task.interval_hours)
            self.update_task_in_db(task)
            logger.error(f"âŒ ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥: {error_msg}")
    
    def fetch_holders_data(self, task: HolderCollectionTask) -> List[Dict]:
        """è·å–holderæ•°æ® - ä½¿ç”¨ç°æœ‰çš„get_all_holderså‡½æ•°"""
        try:
            # è°ƒç”¨åŸæœ‰çš„æ•°æ®è·å–å‡½æ•°
            df = get_all_holders(
                chain_id=task.chain,
                token_address=task.token_address,
                timestamp=None,  # ä½¿ç”¨å½“å‰æ—¶é—´
                top_n=task.max_records
            )
            
            if df.empty:
                logger.warning(f"âš ï¸ æœªè·å–åˆ° {task.task_id} çš„holderæ•°æ®")
                return []
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            holders_list = []
            for _, row in df.iterrows():
                holder_data = {
                    'address': row.get('address', ''),
                    'balance': row.get('balance', ''),
                    'percentage': row.get('percentage', 0),
                    'value_usd': row.get('value_usd', 0),
                    'rank': row.get('rank', 0)
                }
                holders_list.append(holder_data)
            
            logger.info(f"âœ… è·å–åˆ° {len(holders_list)} æ¡holderæ•°æ®")
            return holders_list
            
        except Exception as e:
            logger.error(f"âŒ è·å–holderæ•°æ®å¤±è´¥: {e}")
            return []
    
    def save_snapshot(self, task_id: str, holders_data: List[Dict]):
        """ä¿å­˜å¿«ç…§æ•°æ®åˆ°æ•°æ®åº“"""
        if not holders_data:
            return
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        snapshot_time = datetime.datetime.now()
        task = self.tasks[task_id]
        
        # æ‰¹é‡æ’å…¥æ•°æ®
        insert_data = []
        for rank, holder in enumerate(holders_data, 1):
            insert_data.append((
                task_id,
                snapshot_time,
                holder.get('address', ''),
                task.token_address,
                holder.get('balance', ''),
                holder.get('percentage', 0),
                rank,
                holder.get('value_usd', 0)
            ))
        
        cursor.executemany('''
            INSERT INTO holder_snapshots 
            (task_id, snapshot_time, holder_address, token_address, balance, 
             percentage, rank_position, value_usd)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', insert_data)
        
        conn.commit()
        conn.close()
        
        logger.info(f"ğŸ’¾ ä¿å­˜å¿«ç…§æ•°æ®: {len(insert_data)} æ¡è®°å½•")
    
    def update_task_in_db(self, task: HolderCollectionTask):
        """æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡ä¿¡æ¯"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                UPDATE collection_tasks 
                SET last_run = ?, next_run = ?, total_collections = ?, last_error = ?
                WHERE task_id = ?
            ''', (
                task.last_run, task.next_run, task.total_collections, 
                task.last_error, task.task_id
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ä»»åŠ¡ä¿¡æ¯å¤±è´¥: {e}")
    
    def get_tasks(self) -> List[Dict]:
        """è·å–æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨"""
        return [task.to_dict() for task in self.tasks.values()]
    
    def get_task_snapshots(self, task_id: str, limit: int = 100) -> List[Dict]:
        """è·å–ä»»åŠ¡çš„å¿«ç…§æ•°æ®"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT snapshot_time, holder_address, balance, percentage, rank_position, value_usd
                FROM holder_snapshots 
                WHERE task_id = ?
                ORDER BY snapshot_time DESC, rank_position ASC
                LIMIT ?
            ''', (task_id, limit))
            
            rows = cursor.fetchall()
            columns = ['snapshot_time', 'holder_address', 'balance', 'percentage', 'rank_position', 'value_usd']
            
            result = []
            for row in rows:
                result.append(dict(zip(columns, row)))
            
            conn.close()
            return result
            
        except Exception as e:
            logger.error(f"âŒ è·å–å¿«ç…§æ•°æ®å¤±è´¥: {e}")
            return []
    
    def export_task_data(self, task_id: str, output_path: str = None) -> str:
        """å¯¼å‡ºä»»åŠ¡æ•°æ®ä¸ºCSV"""
        try:
            conn = sqlite3.connect(self.db_path)
            
            # è·å–æ‰€æœ‰å¿«ç…§æ•°æ®
            df = pd.read_sql_query('''
                SELECT task_id, snapshot_time, holder_address, token_address, 
                       balance, percentage, rank_position, value_usd
                FROM holder_snapshots 
                WHERE task_id = ?
                ORDER BY snapshot_time DESC, rank_position ASC
            ''', conn, params=(task_id,))
            
            conn.close()
            
            if output_path is None:
                output_path = f"holder_data_{task_id}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
            logger.info(f"ğŸ“ æ•°æ®å·²å¯¼å‡º: {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºæ•°æ®å¤±è´¥: {e}")
            return None
    
    def start_scheduler(self):
        """å¯åŠ¨å®šæ—¶è°ƒåº¦å™¨"""
        if self.is_running:
            logger.warning("âš ï¸ è°ƒåº¦å™¨å·²åœ¨è¿è¡Œ")
            return
        
        self.is_running = True
        
        def run_scheduler():
            logger.info("ğŸ”„ å®šæ—¶è°ƒåº¦å™¨å¯åŠ¨")
            while self.is_running:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            logger.info("â¹ï¸ å®šæ—¶è°ƒåº¦å™¨åœæ­¢")
        
        self.scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
        self.scheduler_thread.start()
        
        logger.info("âœ… å®šæ—¶è°ƒåº¦å™¨å·²å¯åŠ¨")
    
    def stop_scheduler(self):
        """åœæ­¢å®šæ—¶è°ƒåº¦å™¨"""
        self.is_running = False
        if self.scheduler_thread and self.scheduler_thread.is_alive():
            self.scheduler_thread.join(timeout=5)
        logger.info("â¹ï¸ å®šæ—¶è°ƒåº¦å™¨å·²åœæ­¢")


# ä»¥ä¸‹ä¿ç•™åŸæœ‰çš„æ•°æ®è·å–å‡½æ•°
def fetch_okx_data(url, params=None, timeout=15):
    """ä¸“é—¨ç”¨äºOKX APIçš„è¯·æ±‚å‡½æ•°"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/json, text/plain, */*',
        'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Referer': 'https://www.okx.com/',
        'Origin': 'https://www.okx.com',
        'Sec-Fetch-Dest': 'empty',
        'Sec-Fetch-Mode': 'cors',
        'Sec-Fetch-Site': 'same-origin'
    }
    
    try:
        print(f"ğŸŒ å‘é€è¯·æ±‚åˆ°: {url}")
        print(f"ğŸ“ è¯·æ±‚å‚æ•°: {params}")
        
        # æ£€æŸ¥å‚æ•°ä¸­æ˜¯å¦åŒ…å«æ—¶é—´æˆ³ï¼Œå¹¶æ ¼å¼åŒ–æ˜¾ç¤º
        if params and ('timestamp' in params or 't' in params):
            ts = params.get('timestamp', params.get('t'))
            if ts:
                dt_str = datetime.datetime.fromtimestamp(int(ts)/1000).strftime('%Y-%m-%d %H:%M:%S')
                print(f"â° è¯·æ±‚æ—¶é—´ç‚¹: {dt_str}")
        
        # æ„å»ºå®Œæ•´URL(ä¾¿äºè°ƒè¯•)
        from urllib.parse import urlencode
        full_url = f"{url}?{urlencode(params or {})}"
        print(f"ğŸ”— å®Œæ•´URL: {full_url}")
        
        # ç¡®ä¿ç›´æ¥è¯·æ±‚ OKX API
        response = requests.get(
            url, 
            params=params, 
            headers=headers, 
            timeout=timeout,
            verify=True  # ä½¿ç”¨SSLéªŒè¯
        )
        
        print(f"ğŸ“Š å“åº”çŠ¶æ€ç : {response.status_code}")
        print(f"ğŸ”— å®é™…è¯·æ±‚URL: {response.url}")
        
        response.raise_for_status()
        
        result = response.json()
        print(f"âœ… å“åº”code: {result.get('code')}")
        print(f"ğŸ“„ å“åº”æ¶ˆæ¯: {result.get('msg', 'N/A')}")
        
        return result
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        return None
    except ValueError as e:
        print(f"âŒ JSONè§£æå¤±è´¥: {e}")
        print(f"åŸå§‹å“åº”: {response.text[:500] if 'response' in locals() else 'N/A'}")
        return None
    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
        return None


def get_all_holders(chain_id, token_address, timestamp=None, top_n=100):
    """è·å–æŒ‡å®šæ—¶é—´ç‚¹çš„å‰Nå¤§æŒä»“åœ°å€"""
    # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„ OKX API URL
    url = "https://www.okx.com/priapi/v1/dx/market/v2/holders/ranking-list"
    
    # éªŒè¯å‚æ•°
    if not chain_id or not token_address:
        print("âŒ å‚æ•°é”™è¯¯: chain_id å’Œ token_address ä¸èƒ½ä¸ºç©º")
        return pd.DataFrame()
    
    # å‡†å¤‡å‚æ•°
    params = {
        "chainId": str(chain_id),
        "tokenAddress": token_address,
        "limit": min(top_n, 100),
        "offset": 0
    }
    
    # æ·»åŠ æ—¶é—´æˆ³ - å°è¯•ä¸åŒçš„å‚æ•°åç§°
    if timestamp:
        # OKX APIå¯èƒ½ä½¿ç”¨ä¸åŒçš„å‚æ•°åç§°è·å–å†å²æ•°æ®
        # å°è¯•ä¸åŒçš„å‚æ•°ç»„åˆ
        params["timestamp"] = int(timestamp)  # å¸¸è§å‚æ•°å
        params["snapshotTime"] = int(timestamp)  # å¦ä¸€ç§å¯èƒ½çš„å‚æ•°å
        params["historyTimestamp"] = int(timestamp)  # å¦ä¸€ç§å¯èƒ½çš„å‚æ•°å
        params["time"] = int(timestamp)  # ç®€åŒ–çš„å‚æ•°å
        params["t"] = int(timestamp)  # åŸå§‹å‚æ•°ï¼Œä¿ç•™å…¼å®¹æ€§
        
        # å¢åŠ æ—¥å¿—ä»¥ä¾¿è°ƒè¯•
        dt_str = datetime.datetime.fromtimestamp(timestamp/1000).strftime('%Y-%m-%d %H:%M:%S')
        print(f"ğŸ“† æŸ¥è¯¢å†å²æ—¶é—´ç‚¹: {dt_str} (timestamp: {timestamp})")
    else:
        current_time = int(time.time() * 1000)
        params["t"] = current_time
    
    print(f"ğŸ¯ å¼€å§‹è·å–æŒä»“æ•°æ®...")
    print(f"ğŸ”— é“¾ID: {chain_id}")
    print(f"ğŸ’° ä»£å¸åœ°å€: {token_address}")
    print(f"ğŸ“Š ç›®æ ‡æ•°é‡: {top_n}")
    
    all_holders = []
    page_count = 0
    max_pages = 20
    
    try:
        while len(all_holders) < top_n and page_count < max_pages:
            print(f"\nğŸ“„ æ­£åœ¨è¯·æ±‚ç¬¬ {page_count + 1} é¡µï¼Œå·²è·å– {len(all_holders)} æ¡æ•°æ®")
            
            # ä½¿ç”¨ä¸“é—¨çš„è¯·æ±‚å‡½æ•°
            response = fetch_okx_data(url, params)
            
            if not response:
                print("âŒ APIå“åº”ä¸ºç©ºï¼Œåœæ­¢è¯·æ±‚")
                break
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            response_code = response.get('code')
            if response_code != 0:
                error_msg = response.get('error_message') or response.get('msg') or 'Unknown error'
                print(f"âŒ APIè¿”å›é”™è¯¯: code={response_code}, message={error_msg}")
                break
                
            # è·å–æŒä»“åˆ—è¡¨
            data_obj = response.get('data', {})
            holder_list = data_obj.get('holderRankingList', [])
            
            if not holder_list:
                print("âš ï¸ holderRankingList ä¸ºç©º")
                # æ‰“å°å®Œæ•´å“åº”ä»¥ä¾¿è°ƒè¯•
                print(f"å®Œæ•´å“åº”: {json.dumps(response, indent=2)[:1000]}")
                break
            
            # æ£€æŸ¥å“åº”ä¸­æ˜¯å¦åŒ…å«æ—¶é—´æˆ³æˆ–æ—¥æœŸä¿¡æ¯ï¼Œç”¨äºéªŒè¯APIæ˜¯å¦çœŸçš„è¿”å›äº†å†å²æ•°æ®
            response_time = data_obj.get('timestamp') or data_obj.get('snapshotTime') or params.get('timestamp')
            if response_time:
                resp_time_str = datetime.datetime.fromtimestamp(int(response_time)/1000).strftime('%Y-%m-%d %H:%M:%S')
                print(f"ğŸ“… APIè¿”å›æ•°æ®æ—¶é—´ç‚¹: {resp_time_str}")
                
                # æ£€æŸ¥è¿”å›çš„æ—¶é—´ä¸è¯·æ±‚çš„æ—¶é—´æ˜¯å¦åŒ¹é…
                requested_time = params.get('timestamp') or params.get('t')
                if requested_time and abs(int(response_time) - int(requested_time)) > 86400000:  # 24å°æ—¶æ¯«ç§’æ•°
                    print(f"âš ï¸ è­¦å‘Š: APIè¿”å›çš„æ—¶é—´ä¸è¯·æ±‚çš„æ—¶é—´ç›¸å·®è¶…è¿‡24å°æ—¶!")
                    print(f"ğŸ“Š è¯·æ±‚æ—¶é—´: {datetime.datetime.fromtimestamp(int(requested_time)/1000).strftime('%Y-%m-%d %H:%M:%S')}")
                    print(f"ğŸ“Š è¿”å›æ—¶é—´: {resp_time_str}")
            else:
                print("âš ï¸ è­¦å‘Š: APIå“åº”ä¸­æ²¡æœ‰æ—¶é—´æˆ³ä¿¡æ¯ï¼Œæ— æ³•éªŒè¯æ˜¯å¦è¿”å›äº†å†å²æ•°æ®")
            
            print(f"âœ… æœ¬é¡µè·å–åˆ° {len(holder_list)} æ¡æ•°æ®")
            all_holders.extend(holder_list)
            
            # æ£€æŸ¥æ˜¯å¦å·²åˆ°æœ€åä¸€é¡µ
            if len(holder_list) < params['limit']:
                print("ğŸ å·²åˆ°æœ€åä¸€é¡µ")
                break
                
            # æ›´æ–°offsetåˆ°ä¸‹ä¸€é¡µ
            params['offset'] += params['limit']
            page_count += 1
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            print("â³ ç­‰å¾…0.8ç§’...")
            time.sleep(0.8)
    
    except Exception as e:
        print(f"âŒ è·å–æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()
    
    # æ£€æŸ¥ç»“æœ
    if not all_holders:
        print(f"âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•æŒä»“æ•°æ®")
        print(f"ğŸ“ è¯·æ±‚å‚æ•°: chainId={chain_id}, tokenAddress={token_address}")
        return pd.DataFrame()
    
    print(f"ğŸ‰ æ€»å…±è·å–åˆ° {len(all_holders)} æ¡æŒä»“æ•°æ®")
    
    # å¤„ç†æ•°æ®
    try:
        df = pd.DataFrame(all_holders)
        print(f"ğŸ“‹ åŸå§‹æ•°æ®å­—æ®µ: {df.columns.tolist()}")
        
        # æ˜¾ç¤ºç¬¬ä¸€æ¡æ•°æ®
        if len(df) > 0:
            print(f"ğŸ“Š æ•°æ®æ ·ä¾‹:")
            sample_data = df.iloc[0].to_dict()
            for key, value in list(sample_data.items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªå­—æ®µ
                print(f"  {key}: {value}")
        
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        required_fields = ['holderWalletAddress', 'holdAmount', 'holdAmountPercentage']
        missing_fields = [field for field in required_fields if field not in df.columns]
        
        if missing_fields:
            print(f"âš ï¸ ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}")
            print(f"ğŸ“‹ å¯ç”¨å­—æ®µ: {df.columns.tolist()}")
            return df  # è¿”å›åŸå§‹æ•°æ®
        
        # å¤„ç†æ•°æ®
        df_processed = df[required_fields].copy()
        df_processed.columns = ['address', 'balance', 'percentage']
        
        # æ•°æ®ç±»å‹è½¬æ¢
        df_processed['balance'] = pd.to_numeric(df_processed['balance'], errors='coerce')
        df_processed['percentage'] = pd.to_numeric(df_processed['percentage'], errors='coerce')
        
        # æ·»åŠ é¢å¤–å­—æ®µ
        extra_fields = {
            'chainId': 'chain_id',
            'explorerUrl': 'explorer_url',
            'holdCreateTime': 'hold_create_time'
        }
        
        for original_field, new_field in extra_fields.items():
            if original_field in df.columns:
                df_processed[new_field] = df[original_field]
        
        # é™åˆ¶æ•°é‡å¹¶æ’åº
        df_processed = df_processed.head(top_n)
        df_processed = df_processed.sort_values('percentage', ascending=False).reset_index(drop=True)
        
        print(f"âœ… æ•°æ®å¤„ç†å®Œæˆ!")
        print(f"ğŸ“Š å‰5åæŒä»“åœ°å€:")
        print(df_processed[['address', 'balance', 'percentage']].head())
        
        return df_processed
        
    except Exception as e:
        print(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()



# å…¨å±€é‡‡é›†å™¨å®ä¾‹
_global_collector = None

def get_collector() -> HolderDataCollector:
    """è·å–å…¨å±€é‡‡é›†å™¨å®ä¾‹"""
    global _global_collector
    if _global_collector is None:
        _global_collector = HolderDataCollector()
    return _global_collector

def create_collection_task(task_id: str, token_address: str, token_symbol: str, 
                          chain: str, interval_hours: int, max_records: int = 1000,
                          description: str = "") -> bool:
    """åˆ›å»ºæ–°çš„é‡‡é›†ä»»åŠ¡"""
    collector = get_collector()
    
    task = HolderCollectionTask(
        task_id=task_id,
        token_address=token_address,
        token_symbol=token_symbol,
        chain=chain,
        interval_hours=interval_hours,
        max_records=max_records,
        description=description
    )
    
    return collector.add_task(task)

def start_collection_service():
    """å¯åŠ¨é‡‡é›†æœåŠ¡"""
    collector = get_collector()
    collector.start_scheduler()
    logger.info("ğŸš€ Holderæ•°æ®é‡‡é›†æœåŠ¡å·²å¯åŠ¨")

def stop_collection_service():
    """åœæ­¢é‡‡é›†æœåŠ¡"""
    global _global_collector
    if _global_collector:
        _global_collector.stop_scheduler()
    logger.info("â¹ï¸ Holderæ•°æ®é‡‡é›†æœåŠ¡å·²åœæ­¢")

def list_collection_tasks() -> List[Dict]:
    """åˆ—å‡ºæ‰€æœ‰é‡‡é›†ä»»åŠ¡"""
    collector = get_collector()
    return collector.get_tasks()

def get_task_data(task_id: str, limit: int = 100) -> List[Dict]:
    """è·å–ä»»åŠ¡çš„é‡‡é›†æ•°æ®"""
    collector = get_collector()
    return collector.get_task_snapshots(task_id, limit)

def export_task_data_csv(task_id: str, output_path: str = None) -> str:
    """å¯¼å‡ºä»»åŠ¡æ•°æ®ä¸ºCSV"""
    collector = get_collector()
    return collector.export_task_data(task_id, output_path)

def get_all_tasks_summary() -> List[Dict]:
    """è·å–æ‰€æœ‰ä»»åŠ¡çš„æ•°æ®æ¦‚è§ˆ"""
    try:
        collector = get_collector()
        tasks = list_collection_tasks()
        
        tasks_summary = []
        for task in tasks:
            task_id = task['task_id']
            
            # è·å–ä»»åŠ¡åŸºæœ¬ä¿¡æ¯
            summary = {
                'task_id': task_id,
                'token_symbol': task.get('token_symbol', ''),
                'status': task.get('status', ''),
                'interval_hours': task.get('interval_hours', 0),
                'total_collections': task.get('total_collections', 0),
                'latest_collection': task.get('latest_collection', ''),
                'description': task.get('description', ''),
                'recent_data_count': 0,
                'has_data': False
            }
            
            # è·å–æœ€è¿‘æ•°æ®é‡
            try:
                recent_data = get_task_data(task_id, limit=50)
                summary['recent_data_count'] = len(recent_data)
                summary['has_data'] = len(recent_data) > 0
            except:
                pass
            
            tasks_summary.append(summary)
        
        return tasks_summary
    
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡æ¦‚è§ˆå¤±è´¥: {e}")
        return []

def pause_collection_task(task_id: str) -> bool:
    """æš‚åœé‡‡é›†ä»»åŠ¡"""
    collector = get_collector()
    return collector.pause_task(task_id)

def resume_collection_task(task_id: str) -> bool:
    """æ¢å¤é‡‡é›†ä»»åŠ¡"""
    collector = get_collector()
    return collector.resume_task(task_id)

def remove_collection_task(task_id: str) -> bool:
    """åˆ é™¤é‡‡é›†ä»»åŠ¡"""
    collector = get_collector()
    return collector.remove_task(task_id)

def run_task_now(task_id: str):
    """ç«‹å³æ‰§è¡Œä¸€æ¬¡é‡‡é›†ä»»åŠ¡"""
    collector = get_collector()
    collector.run_collection(task_id)

def analyze_holder_patterns(task_id: str, top_n: int = 100, min_snapshots: int = 3) -> Dict:
    """
    åˆ†æå®šæ—¶é‡‡é›†çš„æŒä»“æ•°æ®ï¼Œè¯†åˆ«æ—©æœŸå…¥å±€ä¸”é•¿æœŸæŒä»“çš„åœ°å€ï¼ˆç–‘ä¼¼åº„å®¶ï¼‰
    
    Args:
        task_id: é‡‡é›†ä»»åŠ¡ID
        top_n: åˆ†æå‰NåæŒä»“è€…
        min_snapshots: æœ€å°‘å‡ºç°åœ¨å‡ ä¸ªå¿«ç…§ä¸­æ‰ç®—æŒç»­æŒä»“
    
    Returns:
        Dict: åˆ†æç»“æœ
    """
    try:
        # è·å–ä»»åŠ¡çš„æ‰€æœ‰å¿«ç…§æ•°æ®
        snapshots = get_task_data(task_id, limit=1000)
        
        if not snapshots:
            raise Exception(f"ä»»åŠ¡ {task_id} æš‚æ— é‡‡é›†æ•°æ®")
        
        # æŒ‰æ—¶é—´åˆ†ç»„å¿«ç…§
        snapshots_by_time = {}
        for record in snapshots:
            snapshot_time = record['snapshot_time']
            if snapshot_time not in snapshots_by_time:
                snapshots_by_time[snapshot_time] = []
            snapshots_by_time[snapshot_time].append(record)
        
        if len(snapshots_by_time) < 2:
            raise Exception(f"å¿«ç…§æ•°é‡ä¸è¶³ï¼Œéœ€è¦è‡³å°‘2ä¸ªæ—¶é—´ç‚¹çš„æ•°æ®ï¼Œå½“å‰åªæœ‰ {len(snapshots_by_time)} ä¸ª")
        
        # åˆ†ææ¯ä¸ªå¿«ç…§çš„top_næŒä»“è€…
        time_points = sorted(snapshots_by_time.keys())
        holder_analysis = {}
        
        # ç»Ÿè®¡æ¯ä¸ªåœ°å€åœ¨ä¸åŒå¿«ç…§ä¸­çš„è¡¨ç°
        address_snapshots = {}  # {address: {time: {rank, percentage, balance}}}
        
        for snapshot_time in time_points:
            snapshot_holders = snapshots_by_time[snapshot_time]
            
            # æŒ‰æŒä»“æ¯”ä¾‹æ’åºï¼Œå–top_n
            snapshot_holders.sort(key=lambda x: float(x.get('percentage', 0)), reverse=True)
            top_holders = snapshot_holders[:top_n]
            
            for rank, holder in enumerate(top_holders, 1):
                address = holder['holder_address']
                
                if address not in address_snapshots:
                    address_snapshots[address] = {}
                
                address_snapshots[address][snapshot_time] = {
                    'rank': rank,
                    'percentage': float(holder.get('percentage', 0)),
                    'balance': holder.get('balance', '0'),
                    'value_usd': float(holder.get('value_usd', 0))
                }
        
        # åˆ†æä¸åŒç±»å‹çš„åœ°å€
        persistent_whales = []      # æ—©æœŸå…¥å±€ä¸”é•¿æœŸæŒä»“ï¼ˆç–‘ä¼¼åº„å®¶ï¼‰
        frequent_traders = []       # é¢‘ç¹è¿›å‡ºçš„åœ°å€ï¼ˆæ¬ç –å…šï¼‰
        new_entrants = []          # æ–°å…¥åœºçš„å¤§æˆ·
        disappeared_holders = []    # å·²ç»é€€å‡ºçš„åœ°å€
        
        for address, time_data in address_snapshots.items():
            snapshot_count = len(time_data)
            time_list = sorted(time_data.keys())
            
            # è®¡ç®—æŒä»“ç¨³å®šæ€§
            if snapshot_count >= min_snapshots:
                # æ£€æŸ¥æ˜¯å¦æ—©æœŸå°±åœ¨æ¦œä¸Š
                earliest_time = time_list[0]
                latest_time = time_list[-1]
                
                earliest_rank = time_data[earliest_time]['rank']
                latest_rank = time_data[latest_time]['rank']
                
                # è®¡ç®—å¹³å‡æ’åå’Œæ’åæ³¢åŠ¨
                ranks = [data['rank'] for data in time_data.values()]
                avg_rank = sum(ranks) / len(ranks)
                rank_volatility = max(ranks) - min(ranks)
                
                # è®¡ç®—æŒä»“æ¯”ä¾‹å˜åŒ–
                percentages = [data['percentage'] for data in time_data.values()]
                avg_percentage = sum(percentages) / len(percentages)
                percentage_change = percentages[-1] - percentages[0] if len(percentages) > 1 else 0
                
                # åˆ¤æ–­åœ°å€ç±»å‹
                analysis_data = {
                    'address': address,
                    'snapshot_count': snapshot_count,
                    'first_seen': earliest_time,
                    'last_seen': latest_time,
                    'earliest_rank': earliest_rank,
                    'latest_rank': latest_rank,
                    'avg_rank': round(avg_rank, 1),
                    'rank_volatility': rank_volatility,
                    'avg_percentage': round(avg_percentage, 4),
                    'percentage_change': round(percentage_change, 4),
                    'latest_percentage': percentages[-1],
                    'latest_balance': time_data[latest_time]['balance'],
                    'latest_value_usd': time_data[latest_time]['value_usd'],
                    'stability_score': round((snapshot_count / len(time_points)) * (1 - rank_volatility / 100), 3)
                }
                
                # æ—©æœŸå…¥å±€ä¸”é•¿æœŸæŒä»“ï¼ˆç–‘ä¼¼åº„å®¶ç‰¹å¾ï¼‰
                if (earliest_rank <= 20 and  # æ—©æœŸå°±åœ¨å‰20
                    rank_volatility <= 30 and  # æ’åå˜åŒ–ä¸å¤§
                    percentage_change >= -0.5):  # æŒä»“æ¯”ä¾‹æ²¡æœ‰å¤§å¹…å‡å°‘
                    analysis_data['whale_type'] = 'ç–‘ä¼¼åº„å®¶'
                    analysis_data['confidence'] = 'high' if rank_volatility <= 15 else 'medium'
                    persistent_whales.append(analysis_data)
                
                # é¢‘ç¹è¿›å‡ºï¼ˆæ¬ç –å…šç‰¹å¾ï¼‰
                elif rank_volatility > 50:  # æ’åæ³¢åŠ¨å¾ˆå¤§
                    analysis_data['whale_type'] = 'æ¬ç –åœ°å€'
                    analysis_data['confidence'] = 'high' if rank_volatility > 80 else 'medium'
                    frequent_traders.append(analysis_data)
                
                # æ–°å…¥åœºå¤§æˆ·
                elif earliest_time in time_list[-3:] and latest_rank <= 50:  # æœ€è¿‘æ‰å‡ºç°ä½†æ’åé å‰
                    analysis_data['whale_type'] = 'æ–°å…¥åœºå¤§æˆ·'
                    analysis_data['confidence'] = 'high' if latest_rank <= 20 else 'medium'
                    new_entrants.append(analysis_data)
            
            # å·²æ¶ˆå¤±çš„åœ°å€ï¼ˆåœ¨æ—©æœŸå¿«ç…§ä¸­å‡ºç°ä½†æœ€è¿‘æ¶ˆå¤±ï¼‰
            elif snapshot_count >= 2 and time_list[-1] not in time_list[-2:]:
                latest_data = time_data[time_list[-1]]
                disappeared_holders.append({
                    'address': address,
                    'last_seen': time_list[-1],
                    'last_rank': latest_data['rank'],
                    'last_percentage': latest_data['percentage'],
                    'snapshot_count': snapshot_count
                })
        
        # æŒ‰ç›¸å…³æŒ‡æ ‡æ’åº
        persistent_whales.sort(key=lambda x: (x['stability_score'], -x['avg_rank']), reverse=True)
        frequent_traders.sort(key=lambda x: x['rank_volatility'], reverse=True)
        new_entrants.sort(key=lambda x: x['latest_rank'])
        disappeared_holders.sort(key=lambda x: x['last_rank'])
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        analysis_result = {
            'task_id': task_id,
            'total_snapshots': len(time_points),
            'time_range': {
                'start': time_points[0],
                'end': time_points[-1]
            },
            'total_addresses_analyzed': len(address_snapshots),
            'persistent_whales': persistent_whales[:20],  # å‰20ä¸ªç–‘ä¼¼åº„å®¶
            'frequent_traders': frequent_traders[:15],    # å‰15ä¸ªæ¬ç –åœ°å€  
            'new_entrants': new_entrants[:10],           # å‰10ä¸ªæ–°å…¥åœºå¤§æˆ·
            'disappeared_holders': disappeared_holders[:10],  # å‰10ä¸ªæ¶ˆå¤±åœ°å€
            'summary': {
                'suspected_whales_count': len(persistent_whales),
                'active_traders_count': len(frequent_traders),
                'new_big_holders_count': len(new_entrants),
                'disappeared_count': len(disappeared_holders)
            }
        }
        
        logger.info(f"âœ… æŒä»“æ¨¡å¼åˆ†æå®Œæˆ: {task_id}")
        logger.info(f"   ç–‘ä¼¼åº„å®¶: {len(persistent_whales)}ä¸ª")
        logger.info(f"   æ¬ç –åœ°å€: {len(frequent_traders)}ä¸ª") 
        logger.info(f"   æ–°å…¥åœºå¤§æˆ·: {len(new_entrants)}ä¸ª")
        
        return analysis_result
        
    except Exception as e:
        logger.error(f"âŒ æŒä»“æ¨¡å¼åˆ†æå¤±è´¥: {e}")
        raise

# ç¤ºä¾‹ç”¨æ³•
def example_usage():
    """ç¤ºä¾‹ç”¨æ³•"""
    print("\n" + "="*60)
    print("ğŸ“– Holderæ•°æ®è‡ªåŠ¨é‡‡é›†ç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹")
    print("="*60)
    
    # 1. åˆ›å»ºé‡‡é›†ä»»åŠ¡
    print("\n1. åˆ›å»ºé‡‡é›†ä»»åŠ¡...")
    success = create_collection_task(
        task_id="BONK_holders",
        token_address="6FtbGaqgZzti1TxJksBV4PSya5of9VqA9vJNDxPwbonk",
        token_symbol="BONK",
        chain="501",  # Solana
        interval_hours=4,  # æ¯4å°æ—¶é‡‡é›†ä¸€æ¬¡
        max_records=100,
        description="BONKä»£å¸æŒä»“è€…æ•°æ®é‡‡é›†"
    )
    print(f"ä»»åŠ¡åˆ›å»ºç»“æœ: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}")
    
    # 2. å¯åŠ¨é‡‡é›†æœåŠ¡
    print("\n2. å¯åŠ¨é‡‡é›†æœåŠ¡...")
    start_collection_service()
    
    # 3. åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡
    print("\n3. å½“å‰é‡‡é›†ä»»åŠ¡:")
    tasks = list_collection_tasks()
    for i, task in enumerate(tasks, 1):
        print(f"  {i}. {task['task_id']} - {task['token_symbol']} ({task['status']})")
        print(f"     é—´éš”: {task['interval_hours']}å°æ—¶, å·²é‡‡é›†: {task['total_collections']}æ¬¡")
    
    # 4. ç«‹å³æ‰§è¡Œä¸€æ¬¡é‡‡é›†
    print("\n4. ç«‹å³æ‰§è¡Œä¸€æ¬¡é‡‡é›†...")
    if tasks:
        task_id = tasks[0]['task_id']
        run_task_now(task_id)
        
        # æŸ¥çœ‹é‡‡é›†æ•°æ®
        print("\n5. æŸ¥çœ‹é‡‡é›†æ•°æ®...")
        data = get_task_data(task_id, limit=10)
        print(f"è·å–åˆ° {len(data)} æ¡æ•°æ®")
        
        # å¯¼å‡ºæ•°æ®
        print("\n6. å¯¼å‡ºæ•°æ®...")
        csv_path = export_task_data_csv(task_id)
        print(f"æ•°æ®å·²å¯¼å‡ºåˆ°: {csv_path}")
    
    print("\nâœ… ç¤ºä¾‹å®Œæˆï¼")
    print("ğŸ’¡ æç¤º: é‡‡é›†æœåŠ¡å°†åœ¨åå°æŒç»­è¿è¡Œï¼ŒæŒ‰è®¾å®šé—´éš”è‡ªåŠ¨é‡‡é›†æ•°æ®")


if __name__ == "__main__":
    print("ğŸš€ Holderæ•°æ®è‡ªåŠ¨é‡‡é›†ç³»ç»Ÿ")
    
    user_choice = input("\nè¯·é€‰æ‹©æ“ä½œ:\n1. è¿è¡Œç¤ºä¾‹\n2. åŸæœ‰æµ‹è¯•\nè¯·è¾“å…¥é€‰æ‹© (1/2): ")
    
    if user_choice == "1":
        example_usage()
    else:
        # åŸæœ‰çš„æµ‹è¯•ä»£ç 
        print("ğŸš€ å¼€å§‹æµ‹è¯• OKX Holders API...")
        
        # å…ˆæµ‹è¯•è¿æ¥
        if not test_connection():
            print("âŒ ç½‘ç»œè¿æ¥æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®")
            exit(1)
        
        # æµ‹è¯•è·å–æ•°æ®
        print("\n" + "="*60)
        print("ğŸ§ª æµ‹è¯•æ•°æ®è·å–åŠŸèƒ½")
        print("="*60)
        
        test_params = {
            "chain_id": "501",
            "token_address": "6FtbGaqgZzti1TxJksBV4PSya5of9VqA9vJNDxPwbonk",
            "top_n": 10
        }
        
        print(f"ğŸ“ æµ‹è¯•å‚æ•°: {test_params}")
        
        df = get_all_holders(**test_params)
        
        if not df.empty:
            print(f"\nğŸ‰ æµ‹è¯•æˆåŠŸï¼è·å–åˆ° {len(df)} æ¡æŒä»“æ•°æ®")
            
            # å¯¼å‡ºCSV
            csv_filename = f"holders_test_{int(time.time())}.csv"
            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ æ•°æ®å·²å¯¼å‡ºåˆ°: {csv_filename}")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
            print(f"  æ€»æŒä»“åœ°å€æ•°: {len(df)}")
            print(f"  å¹³å‡æŒä»“æ¯”ä¾‹: {df['percentage'].mean():.4f}%")
            print(f"  æœ€å¤§æŒä»“æ¯”ä¾‹: {df['percentage'].max():.4f}%")
            
            # æµ‹è¯•å¤šæ—¶é—´ç‚¹å¿«ç…§åŠŸèƒ½
            user_input = input("\næ˜¯å¦æµ‹è¯•å¤šæ—¶é—´ç‚¹å¿«ç…§åŠŸèƒ½? (y/n): ")
            if user_input.lower() in ('y', 'yes'):
                test_historical_snapshots()
            
        else:
            print("âŒ æµ‹è¯•å¤±è´¥ï¼Œæœªè·å–åˆ°æ•°æ®")
            print("\nğŸ” è¯·æ£€æŸ¥:")
            print("  1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
            print("  2. ä»£å¸åœ°å€æ˜¯å¦æ­£ç¡®")
            print("  3. æ˜¯å¦è¢«APIé™åˆ¶è®¿é—®")